% \documentclass[10]{article}

% standard packages

% A more pleasant font
\usepackage[T1]{fontenc} % use postscript type 1 fonts
\usepackage{textcomp} % use symbols in TS1 encoding
% \usepackage{mathptmx,helvet,courier} % use nice, standard fonts for roman, sans and monospace respectively

% Improves the text layout
\usepackage{microtype}

\usepackage{lscape}
\usepackage{fancyhdr}
% \usepackage{epsfig}
\usepackage{subfigure}
\usepackage{url}
\usepackage{graphics}
% \usepackage{enumerate}
\usepackage{ifthen}
\usepackage{float}
\usepackage{listings}
\lstset{basicstyle=\ttfamily}
% \usepackage[strings]{underscore}
% \usepackage{underscore}
% \usepackage[bookmarks=true,bookmarksnumbered=true,pdfborder={0 0 0}]{hyperref}

\lstset{
  literate={\_}{}{0\discretionary{\_}{}{\_}}%
}

% \usepackage[table]{xcolor}
% \usepackage{booktabs}

\DeclareUrlCommand\email{}

\pagestyle{plain}

\newfloat{listing}{tbp}{lol}
\floatname{listing}{Listing}

\begin{document}
\title{How to Avoid OOTA Without Really Trying}

\newcommand{\co}[1]{\lstinline[breaklines=yes,breakatwhitespace=yes]{#1}}

% Blind review process.
% \author{
% Alan Stern\\{\small \email{stern@rowland.harvard.edu}} \and
% Paul E.~McKenney\\{\small \email{paulmck@kernel.org}} \and
% Michael Wong\\{\small \email{fraggamuffin@gmail.com}} \and
% Maged Michael\\{\small \email{maged.michael@gmail.com}}
% }

\begin{abstract}
	Attempts to create memory models that forbid out-of-thin-air
	(OOTA) behaviors of C++ \co{memory_order_relaxed} accesses
	have been either non-executable, complex, or unloved by
	implementers.
	At the same time, we know of no instances of OOTA behavior in real C++
	implementations.

	We focus on shared-memory programs and C++ implementations based
	on traditional compilers and hardware, including CPUs and GPGPUs,
	thus enabling analysis of semantic dependencies and OOTA behaviors
	by means of existing hardware models.
	We show that these models' constraints prevent OOTA cycles from
	occurring in undefined-behavior-free C++ programs running on
	such implementations, provided the cycles involve only volatile
	atomics.
	We accommodate nonvolatile atomics by defining ``quasi-volatile''
	behavior that enables implementations doing per-thread analysis
	and optimization not only to avoid OOTA, but also to avoid 
	incorrectly evaluating arithmetic expressions involving atomic
	accesses.

	It follows that enforcing OOTA avoidance requires no special
	action from implementations and no changes to user code or
	to the standard.
\end{abstract}
\maketitle{}

\section{Introduction and Background}
\label{sec:Introduction}

We begin with a brief overview of the OOTA problem followed by an equally
brief summary of prior OOTA work.

\subsection{Brief OOTA Overview}
\label{sec:Brief OOTA Overview}

\begin{listing}[bp]
% Constructed by hand.
{
\small
\begin{tabular}{r|l|l}
& \multicolumn{1}{c}{T0} & \multicolumn{1}{c}{T1} \\
\hline
1. &
\texttt{~~int r1 = x} &
	\texttt{~~int r2 = y} \\
2. &
\texttt{~~y = r1} &
	\texttt{~~x = r2} \\
\end{tabular}

\vspace{0.1in}
Condition (0:r1=42 $\wedge$ 1:r2=42)
sometimes satisfied.
}
\caption{Simple OOTA}
\label{lst:Simple OOTA}
\end{listing}

In broad terms, OOTA occurs when a group of threads load from each others'
stores and each thread's store depends on the value returned by that
thread's load.
The collection of loads and stores forms an \emph{OOTA cycle}.
In the most extreme cases a nonsensical value can pop up ``out of thin air''
as shown in
Listing~\ref{lst:Simple OOTA}.
Here, all of \co{x}, \co{y}, \co{r1}, and \co{r2} might have final
values of 42, despite there being no instances of 42 in the default-zero
initial values or in the executable
code~\cite{PaulEMcKenney2020RelaxedGuideRelaxed}.\footnote{
	Throughout this paper, variables whose names are ``\co{r}''
	followed by a number denote private per-thread variables
	(``registers''),
	and other names denote shared variables.
	All shared-variable accesses are relaxed atomic.}
First, thread T0's line~1 does a load from \co{x} into \co{r1},
reading the value of T1's line-2 store rather than \co{x}'s initial
value of zero and somehow obtaining 42.
Second, T0's line~2 stores \co{r1}, and thus 42, to \co{y}.
Third, T1's line~1 loads 42 from \co{y} to \co{r2}.
Finally, T1's line~2 stores 42 to \co{x}, justifying the value loaded
by T0's line~1.

Because nothing else in the C++ memory model rules out such OOTA cycles
(as indicated by the ``sometimes satisfied'' in the figure), the C++
standard explicitly prohibits them in 33.5.4p8
(\co{[atomics.order]})~\cite{ThomasKoeppe2023N4950}:
\begin{quote}
	Implementations should ensure that no “out-of-thin-air” values
	are computed that circularly depend on their own computation.
\end{quote}
This prohibition prevents misapplication of the as-if rule in oracular
C++ implementations.

This paper uses \emph{full C++} to denote the standard including this
prohibition of OOTA and \emph{loose C++} to denote a hypothetical standard
that excludes this prohibition but is otherwise identical to full C++.
Unqualified \emph{C++} means full C++.

In Listing~\ref{lst:Simple OOTA}
there is a \emph{semantic dependency} from line~1 to line~2 in both
T0 and T1.
(Roughly speaking, there is a semantic dependency from a given load
to a given store when \emph{all other things being equal, a change in the
value loaded can change the value stored or prevent the store from
occurring at all.}
Here the dependencies are trivial because the values stored simply
\emph{are} the values that were loaded.)
Since real-world CPUs cannot store something
until they have determined its value,\footnote{
	Another way of saying this is that real-world CPUs do not
	make their stores visible to other CPUs until those stores
	are no longer speculative.
	A similar restriction applies to compiler-based speculation.}
the stores on line~2 cannot take place until T0's and T1's CPUs
know what values are returned by the loads on line~1.
Thus the hardware orders these stores after their corresponding loads,
and this ordering prevents the OOTA result.

\begin{listing}[bp]
% Constructed by hand.
{
\small
\begin{tabular}{r|l|l}
& \multicolumn{1}{c}{T0} & \multicolumn{1}{c}{T1} \\
\hline
1. &
\texttt{~~int r1 = x} &
	\texttt{~~int r2 = y} \\
2. &
\texttt{~~y = r1} &
	\texttt{~~x = 42} \\
\end{tabular}

\vspace{0.1in}
Condition (0:r1=42 $\wedge$ 1:r2=42)
sometimes satisfied.
}
\caption{Simple Reordering}
\label{lst:Simple Reordering}
\end{listing}

Simple reordering can cause non-OOTA, but OOTA-like, behavior, as
exemplified by
Listing~\ref{lst:Simple Reordering}~\cite{PaulEMcKenney2020RelaxedGuideRelaxed}.
Either the compiler or the CPU might reorder T1's lines~1 and~2, so that
all of \co{x}, \co{y}, \co{r1}, and \co{r2} might end up with the value 42,
as indicated by the ``sometimes satisfied'' in the figure.

\begin{listing}[tbp]
@@ DisplayRunLitmus litmus/oota-ctrl.litmus @@
\caption{OOTA Cycle}
\label{lst:OOTA Cycle}
\end{listing}

On the other hand, OOTA cycles need not involve nonsensical or
unmotivated values, as shown in
Listing~\ref{lst:OOTA Cycle}.
Line~3 of both threads stores the value 42 only when line~2 determines
that the value loaded was 42.
As with Listing~\ref{lst:Simple OOTA}, hardware ordering prevents the
OOTA result, as discussed further in
Section~\ref{sec:Hardware Dependencies, Instruction Ordering, and the Fundamental Property}.

\subsection{Prior Work}
\label{sec:Prior Work}

All OOTA workers owe a debt of gratitude to the foundational work led
by William Pugh, represented by the infamous Java-language
``Causality Test Cases''.\footnote{
	\url{http://www.cs.umd.edu/~pugh/java/memoryModel/unifiedProposal/testcases.html}.}
% %%% Eventually cite P3064.

Some executable C++ memory models correctly flag some OOTA
cycles~\cite{JadeAlglave2014HerdingCats}.\footnote{
	Others avoid OOTA by forbidding atomic stores of nonconstant
	values~\cite{MarkBatty2011cppmem}.}
% @@@ Better herd7 C11 citation?
However, because these models are atemporal, they cannot reject
OOTA executions other than by flagging the OOTA value as arbitrary,
which some in fact do in at least some cases.

P0442R0 (``Out-of-Thin-Air Execution is Vacuous'')~\cite{PaulEMcKenney2016OOTA}
provided a decision procedure for distinguishing between reordering and
OOTA, using a perturbation method based on the insight that OOTA cycles
are fixed-point computations.

Some workers recommend avoiding OOTA by forcing prior relaxed
loads to be ordered before subsequent relaxed
stores~\cite{Boehm:2014:OGA:2618128.2618134,HansBoehm2019OOTArevisitedAgain,Lahav:2017:RSC:3062341.3062352},
but this can require executing real
instructions~\cite[Section 7.1]{Maranget2012TutorialARMPower},
consuming real time and real electrical power to solve a strictly
theoretical problem.
% This might have been acceptable in the 1960s of some of the authors'
% youths, but it is now the year 2024.

Other workers recommend various procedures to identify and avoid OOTA
cycles~\cite{Lahav:2017:RSC:3062341.3062352,Sinclair:2017:CAR:3079856.3080206,Lee:10.1145/3385412.3386010,MarkBatty2019ModularRelaxedDependenciesOOTA},
but none of these have been accepted by C++ implementers.
As of 2024, Mark Batty is persisting with modular relaxed dependencies,
which may have the added ability of checking compiler
optimizations~\cite{JayRichards2024SymbolicMrder}.

Goldblatt looked at interactions between OOTA and
undefined behavior (UB)~\cite{DavidGoldblatt2019NoElegantOOTAfix}.
% Appendix~\ref{app:Aside on Undefined Behavior}
% analyzes his examples and notes ways to separate UB and OOTA-cycle
% concerns.
We consider only UB-free examples.

All this work focused on either identifying OOTA or helping C++
implementations to avoid it.
None applied real-world hardware ordering constraints to the problem
of avoiding OOTA cycles.
This paper's contribution is to show how these constraints prevent
real-world C++ implementations from exhibiting OOTA behavior.

\section{OOTA and Semantic Dependencies (sdep)}
\label{sec:OOTA and Semantic Dependencies (sdep)}

This section analyzes the main components of OOTA cycles, namely
semantic dependencies and reads-from links.

\subsection{OOTA: rf versus rfe}
\label{sec:OOTA: rf versus rfe}

Semantic dependencies form only one type of link in an OOTA cycle.
The other type extends from a given store to a load that returns the
value stored.
It is tempting to argue that the store must precede
the load in global time and combine this with the intuitive notion that
any real C++ implementation must consume global time when computing a
semantic dependency.
This combination suggests that OOTA cycles cannot occur.
The idea has been formalized by defining an OOTA cycle as a cycle
in sdep $\cup$ rf~\cite{PaulEMcKenney2014OOTA},
% This citation credits the formulation to Ali Sezgin, third author
% of N4323 ("Out-of-Thin-Air Execution is Vacuous").
where sdep is the set of semantic dependencies within each thread and
rf is the set of store-to-load ``reads-from'' links, whether internally
within a thread (rfi) or externally between threads (rfe).
% @@@ Consider adding citation defining rf, rfi, and rfe.

This is a fine definition and is consistent with the words in the C++
standard, but it has a problem with intrathread rfi links
as exemplified by the following code: \\
% \begin{quote}
% Constructed by hand.
{
\small
\begin{tabular}{rl}
% & \multicolumn{1}{c}{T0} \\
% \hline
1. &
\texttt{~~int r2 = y} \\
2. &
\texttt{~~x = r2} \\
3. &
\texttt{~~int r3 = x} \\
4. &
\texttt{~~z = r3} \\
\end{tabular}
} \\
% \end{quote}
This is an elaboration of T1 from
Listing~\ref{lst:Simple OOTA}
that adds \co{z} along with lines~3 and~4.
The problem is that a C++ implementation may
note that line~3 could well execute immediately after line~2, giving
other threads no chance to modify \co{x} in between.
Such an implementation might therefore behave as if line~3 had been
removed from the source code and line~4's \co{r3} had been replaced by
\co{r2},
causing line~4 always to store the same value to \co{z} as was stored
to \co{x} by line~2.
And since the load from \co{x} has been removed, it cannot possibly
act as a temporal constraint.

To avoid this issue, we will substitute rfe
for rf, defining an OOTA cycle---for now---as a cycle in sdep $\cup$ rfe.
Any rfi links in a cycle can instead be interpreted as part of sdep.
Although this does shunt additional complexity onto the term
``semantic dependency'', it also enables us to cleanly separate
the interthread and intrathread portions of any given OOTA cycle.
% \footnote{
%	Thanks to Alan Stern for providing a litmus test that demonstrated
%	that we had unwittingly (but productively!) shifted our definition
%	to sdep $\cup$ rfe.}

% We no longer need this.  I will move A.1 to the cbmc appendix.
% Please see
% Appendix~\ref{app:Informal Definition of Semantic Dependency}
% for more a more detailed (albeit quite informal) definition.
% Maybe eventually have an appendix that lists all the definitions
% of both ``OOTA cycle'' and ``semantic dependency''.

The inability of rfi links to act as temporal constraints is not the
only, or even the main, weakness in the intuitive argument against
OOTA cycles.
The primary difficulty lies in the fact that the code transformations
performed by optimizing compilers can destroy dependencies, including
semantic ones (depending on one's definition).
That is, even when the potential for a dependency exists in the
source code for a thread, there might be no dependency in the machine
code produced by a compiler.
There would then be no constraint forcing the implementation to execute
the thread's store later in global time than the load it supposedly depends on,
and thus no impediment to the occurrence of an OOTA cycle.
We will see examples of this destruction later on.

\subsection{Properties of Semantic Dependencies}
\label{sec:Properties of Semantic Dependencies}

Here we consider some of the complexities of semantic dependency.

\subsubsection{Semantic Dependencies and Source Code}
\label{sec:Semantic Dependencies and Source Code}

Some discussions of semantic dependencies assume that they
are strictly functions of the source code.
Although there are ways of making this work, many instances of
semantic dependency must be considered functions of particular executions.
Consider for example:
\begin{quote}
\begin{verbatim}
x = y * z;
\end{verbatim}
\end{quote}

As long as \co{z} is zero, changes in the
value of \co{y} will not cause a change in the value stored to \co{x}.
As a result, the semantic dependency from \co{y} to \co{x} exists only
in executions where \co{z} is nonzero,
which shows it is a property of the execution, not just of the source code.

\subsubsection{Semantic Dependencies Can Be Many-To-One}
\label{sec:Semantic Dependencies Can Be Many-To-One}

Suppose that in some execution of the previous example,
both \co{y} and \co{z} are zero.
Then changes to either \co{y} or \co{z}
will not cause a change in the value stored to \co{x}.
In other words, in this execution there is no semantic dependency
from either \co{y} or \co{z} to \co{x}.
But there \emph{is} a semantic dependency from the \emph{pair}
\{\co{y},~\co{z}\} to \co{x},
because changes to both \co{y} and \co{z}
can cause the value stored in \co{x} to change.
This means that, prior work~\cite{PaulEMcKenney2016OOTA}
notwithstanding, accurate definitions of sdep cannot always rely on
single-variable perturbations;
they must at times consider changes to multiple variables.
% @@@  Pull anything in?  Doesn't seem necessary.
% See Appendix~\ref{app:Non-Trivial Semantic Dependencies}
% for examples and additional discussion.

Since we can no longer regard sdep as always relating a single load to a store,
the notion of a cycle involving sdep appears problematic.
We are forced to change our definition of an OOTA cycle again;
we will say that an execution is an instance of OOTA if in that execution:
\begin{quote}
	There are stores $W_0,$ \ldots,~$W_m$,
	where each $W_i$ semantically depends on a set of loads
	$\{R_{i,0},$ \ldots,~$R_{i,n_i}\}$,
	such that each $R_{i,j}$ reads from one of the $W_k$
	stores in a different thread.
\end{quote}
This can make OOTA more complicated than a simple cycle but
we will continue to refer to ``OOTA cycles'' out of habit.
Note that this new definition includes and generalizes the earlier
``cycle in sdep $\cup$ rfe'' definition.

\subsubsection{Non-Deterministic Semantic Dependencies}
\label{sec:Non-Deterministic Semantic Dependencies}

Compiler optimizers also exploit the nondeterministic aspects of an
execution.
Consider this example, with \co{i} initially zero:
\begin{quote}
\begin{verbatim}
int foo(int a, int b)
{
   return a / b;
}

r1 = foo(++i, ++i);
x = r1 * z;
\end{verbatim}
\end{quote}
Because early C~implementers could not come to agreement, the standard
does not specify the order of evaluation of function arguments, so
the value calculated for \co{r1} might be zero ($1/2$ truncated) or two
($2/1$).
In the former case there is no semantic dependency from \co{z} to \co{x},
but in the latter case there is.\footnote{
	Thanks to Peter Sewell for pointing out this possibility.}

(According to the current version of the standard, conflicting side effects
in unsequenced subexpressions constitute undefined behavior,
although there are proposals to make them defined in both
C++~\cite{GabrielDosReis2016P0145r3}
and C~\cite{AlexCeleste2023N3203}.
Nevertheless, the example above is allowed because the order of evaluation
of arguments to a function call is ``indeterminately sequenced''
(7.6.1.3p7 \co{[expr.call]}) rather than unsequenced, a subtle distinction.)

\subsubsection{Semantic Dependencies Affected by Cross-Thread Optimizations}
\label{sec:Semantic Dependencies Affected by Cross-Thread Optimizations}

Consider the following:
\begin{quote}
\begin{verbatim}
x = y - z;
\end{verbatim}
\end{quote}
There appear to be semantic dependencies from \co{y} to \co{x} and from \co{z}
to \co{x}.
However, if the implementation somehow knows that \co{y} is
always equal to \co{z} at this point then there is no semantic dependency;
the implementation can act as if the statement were simply ``\co{x = 0}''.
We leave aside the question of how the implementation would know this,
given that \co{y} and \co{z} cannot be updated simultaneously\footnote{
	At least not by any means within the confines of the standard.}
and are subject to change at any time by
other threads (a point we will return to in
Section~\ref{sec:Global Optimization Can Destroy Dependencies}).
% The usual way in the Linux kernel is via a union, but it is probably
% best to ignore that possibility.

\subsubsection{Semantic Dependencies Affected by \co{if} Statements}
\label{sec:Semantic Dependencies Affected by if Statements}

Consider the following \co{if} statement:
\begin{quote}
\begin{verbatim}
r1 = x;
if (r1 > 0)
    y = r1;
else
    z = r1;
\end{verbatim}
\end{quote}
Here there is a semantic dependency from \co{x}, but in some executions
it extends to \co{y} and in others to \co{z}.
This is an example of a load affecting not only the value of
a given store, but also whether or not that store is executed at all.

\subsubsection{Semantic Dependencies Not Affected by \co{if} Statements}
\label{sec:Semantic Dependencies Not Affected by if Statements}

Compare this example to the previous one:
\begin{quote}
\begin{verbatim}
if (x > 0)
    y = 42;
else
    y = 42;
\end{verbatim}
\end{quote}
Because the stores executed on each arm of the \co{if} statement write
identical values to identical addresses, one could equally well regard
the two statements as performing two different stores or as performing
for all intents and purposes a single store, independent of \co{x}.
Reasonable C++ implementations might disagree on this matter and
therefore on whether or not the example has a semantic dependency.
It is the implementation's choice.

\subsubsection{Semantic Dependencies and Matching Up Stores}
\label{sec:Semantic Dependencies and Matching Up Stores}

Suppose we take the view that the previous example involves only one
store.
This opens up the door to greater complexity:
\begin{quote}
\begin{verbatim}
 1. if (x > 0) {
 2.     y = 42;
 3. } else {
 4.     y = 53;
 5.     y = 42;
 6. }
\end{verbatim}
\end{quote}
Consider an execution in which \co{x} is greater than zero, so line~2
runs.
Is it semantically dependent on \co{x}?
The answer isn't immediately clear.
If the other arm of the \co{if} is taken then a store of the same value 42 to
\co{y} occurs, but 53 is written before it.
Which of these two stores should be compared with the store on line~2?

One way to cut the Gordian knot is to match up the stores by the order
they occur:
Since line~2 is the first store to \co{y} in its arm of the \co{if}
statement, it should be matched up with the first store to \co{y} in
the other arm.
Those two stores write different values so there is a semantic
dependency.

On the other hand, a compiler may decide to drop the \co{y =
53} store entirely, leaving it out of the machine code,
on the grounds that it's always possible for the two adjacent stores
to \co{y} to execute in such quick succession that no other thread
manages to read the value 53 before it gets overwritten with 42.
If the compiler does this then the first store to \co{y} in that
arm of the \co{if} statement \emph{would} agree with the store in
line~2, and so there would not be a semantic dependency.
Once again, the decision is up to the implementation.

\subsubsection{Per-Architecture Dependent Semantic Dependencies}
\label{sec:Per-Architecture Dependent Semantic Dependencies}

The implementation-defined aspects of the standard permit architectures to
differ as to whether an abstract execution contains a semantic dependency.
For example, consider the following, where the type of \co{c} is \co{char}:
\begin{quote}
\begin{verbatim}
y = (c >= 0);
\end{verbatim}
\end{quote}
Here \co{y} is semantically dependent on \co{c} in executions
running on implementations in
which \co{char} is a signed type,
but not those for which it is unsigned.

We have seen several examples showing that semantic dependencies are more
complicated than might first appear, varying according to the execution
and even the implementation.
This raises several questions, of which the first is:
What exactly is an execution?

\section{What is an Execution?}
\label{sec:What is an Execution?}

This section looks more carefully at executions from the viewpoints of
the abstract machine and the computer hardware, and then reconciles
these two viewpoints.

\subsection{Abstract Executions}
\label{sec:Abstract Executions}

The C++ standard describes the execution of a program in terms of
``a parameterized nondeterministic abstract machine'' in 4.1.2p1
(\co{[intro.abstract]}).
This description specifies how the abstract machine carries out the
operations of a source program in great, but not complete, detail:
\begin{itemize}
\item	Some of the abstract machine's characteristics are
	implementation defined, including things like the number of
	bits in the various integer types
	or whether the \co{char} type is signed (see
	Section~\ref{sec:Per-Architecture Dependent Semantic Dependencies}).
\item	Some aspects of an execution are unspecified or nondeterministic,
	including things like the order of evaluation of the operands
	of most binary operators or of the arguments in a function call.
	Implementations may choose from a set of allowed behaviors
	(see
	Section~\ref{sec:Non-Deterministic Semantic Dependencies}).
\item	Some actions are deemed to have undefined results; the standard
	says essentially nothing about programs that can give rise to
	undefined behavior.
\item	Asynchronous actions (i.e., signal handlers) are largely ignored.
\item	Input and output are not described in any detail.
\end{itemize}
In addition to these points, the standard does not specify which store
an atomic load must read from, beyond requiring that the overall
pattern of loads and stores be consistent with the C++ memory model.
In short, the standard grants C++ implementations considerable freedom,
as detailed in
Section~\ref{sec:C++ Compilers}.

The abstract executions we use will be fully specified.
This means that all the missing information must be supplied:
the implementation-defined characteristics, the selections for the
nondeterministic pathways, and most notably, for each load, the store
from which it reads and the value of the load.
We ignore issues of signal handlers and I/O;
in any case our litmus-test programs don't use them
(but see the discussion of volatile loads in
Section~\ref{sec:Relation Between Abstract and Hardware Executions} below).
The totality of this information---along with the program's source
code, of course---determines within each thread a unique, linearly
ordered series of steps to be carried out by the abstract machine.
However, with a few exceptions\footnote{
	Such as a load-acquire synchronizing with a store-release.}
there is no ordering relation between steps carried out
in different threads.
Even if a relaxed atomic load in one thread reads from a relaxed
atomic store in another thread, the standard does not require the
store to come before the load in any meaningful way.

With the compiler-based implementations we are considering,
the choices for the nondeterministic pathways are ``frozen'' into the
machine-code executable file and thus are completely determined
at runtime.
A consequence of this is that if two abstract executions of the same
thread under the same implementation agree on the values obtained by
the load operations during their first $N$ steps then they will agree
in every respect during those steps, although they may diverge later.
(We assume that programs will not indulge in any computations that
could vary spontaneously from one execution to another,
such as basing a dependency on the time of day or a process ID.)

\subsection{Hardware Executions}
\label{sec:Hardware Executions}

The outcome when a given computer executes the machine code in a file
has historically been much better defined than the executions of the
C++ abstract machine.
The hardware's behavior is typically specified with great precision by
the designer or manufacturer, and there are formal, executable memory models
describing exactly what patterns of loads and stores can occur.
Thus, leaving aside asynchronous interrupts and system
calls, the behavior of a CPU executing a particular thread within a
program is entirely determined by the values obtained by its
memory-load instructions.\footnote{
	We regard read-modify-write instructions as consisting of both a
	memory load and a memory store.}

For this reason, the hardware executions we use will comprise (along
with the machine code being run) the computer architecture and for each
load instruction, the store instruction from which it reads and the
value obtained.
At this level, the fact that the original program was in C++ is
irrelevant; the same concepts apply to the execution of a program in
any compiled language.

A computer may execute a thread's instructions out of order.
The architecture specifies the extent to which this may happen, and it
also specifies circumstances under which instructions
must be executed in order.
Nevertheless, we will consider an execution to be determined by the
values obtained by its loads.
As with abstract executions, if two hardware executions of the same
thread on the same type of computer agree on the values obtained by
the load instructions during their first $N$ steps then they will
agree in every respect during those steps, although they may diverge
later.

\subsection{Relation Between Abstract and Hardware Executions}
\label{sec:Relation Between Abstract and Hardware Executions}

The C++ standard requires that for any valid implementation, when a
program runs its observable behavior must be the same as that of some
abstract execution of the source code given the same input (in the
absence of any abstract executions containing undefined behavior).\footnote{
	This requirement is the standard's ``as-if'' rule.}
This means:
(1)~The program's output must be the same as that of the abstract execution;
(2)~Volatile accesses ``are evaluated strictly according to the
rules of the abstract machine'' (4.1.2p6.1 \co{[intro.abstract]}); and
(3)~(A condition on the timing and interleaving of input and output,
which does not matter for our purposes).
We will say that the abstract execution is \emph{realized by} the
hardware execution.

Under any particular implementation,
a single program can have many different abstract executions,
varying in their decisions about which store each load reads from
and thus the value obtained.
It's worth noting, however, that not all the possible abstract executions
of a program need be realizable by the machine-code executable file
produced by that implementation.
In fact, we will see that \emph{none} of the possible OOTA executions
allowed by the loose C++ abstract machine will ever be realized
by the executables produced by many compilers.

Exactly what the standard's restriction on volatile accesses means
isn't entirely clear.
The handling of volatiles, as understood by compiler developers, has
been described as more folklore or a gentlemen's agreement than
anything else.
To help guide C++ users and implementers, the standard adds these
suggestive comments (9.2.9.2p5 and~6 \co{[dcl.type.cv]}):
\begin{quote}
	The semantics of an access through a volatile glvalue are
	implementation-defined.

	\co{volatile} is a hint to the implementation to avoid aggressive
	optimization involving the object because the value of the object
	might be changed by means undetectable by an implementation.
\end{quote}

Taking our cue from the folklore, we propose to recognize formally
that programs with volatile objects can execute in two different kinds
of environment: a benign one in which accesses to these objects work
the same as nonvolatile memory accesses, and a nonbenign one in which
accesses to volatile objects are subject to outside interference and
act more like I/O.
In particular, when it runs in a nonbenign environment, a program's
volatile loads can return unpredictable values.
They don't necessarily read from stores (in contrast to nonvolatile loads,
which always must return the value of the store they read from).
This implies that volatile load-acquires do not synchronize with
volatile store-releases in the sense of the C++ memory model,\footnote{
	However, they might instead synchronize with store-releases in
	device firmware (or vice versa), roughly speaking.}
so they do not contribute to the happens-before relation.
Also, in these environments the rfe relation does not apply to volatile
loads and stores, and hence the accesses in an OOTA cycle must be nonvolatile.

Of course, the machine-code file produced by a compiler must work
properly in either kind of environment.
Therefore the compiler must generally treat accesses to volatile objects
as a form of I/O, and it may not
invent, omit, merge, or reorder these accesses, as we will discuss in
Section~\ref{sec:Volatile and Quasi Volatile Accesses} below.

Given this relation between abstract and hardware executions, it is time
to turn our attention to the tools that manage hardware executions so as
to enforce that relation, namely, compilers.

\section{C++ Compilers}
\label{sec:C++ Compilers}

In addition to a compiler, a complete C++ implementation might include
\co{.h} header files, a linker, runtime libraries, and a dynamic loader.
Nevertheless, for our purposes the compiler is the most important
component because it is what primarily determines the translation from
a C++ source program to directly executable machine code.
We will therefore use ``compiler'' and ``implementation'' interchangeably.

This section discusses influences and constraints on C++ compilers
and then uses this information to define necessary properties of
volatile and \emph{quasi-volatile} atomic accesses.

\subsection{Users Influence the Behavior of Compilers}
\label{sec:Users Influence the Behavior of Compilers}

The exact definition of a computer language is a subject of some debate,
with standards, implementations, and users all having some degree of
influence~\cite{KayvanMemarian2016DepthOfC-1,KayvanMemarian2016DepthOfC-2},
and each being prone to change over time.
In areas that are not well settled or where users might reasonably
want to resist the dictates of the standard,
compilers often provide switches to override their default behaviors.
An example is GCC's \co{-funsigned-char} command-line argument,
which causes it to treat variables of type \co{char} as unsigned.
Many more examples of user control over language semantics
may be found through use of the command \co{make V=1}
in a Linux-kernel source tree and by the discussion
in~\cite{KayvanMemarian2016DepthOfC-1,KayvanMemarian2016DepthOfC-2}.

We will consider these user-specified compiler switch settings to fall
within the implementation-defined parameters of the C++ abstract machine.
They should be provided, implicitly or explicitly, as part of any
abstract execution.

\subsection{Global Optimization Can Destroy Dependencies}
\label{sec:Global Optimization Can Destroy Dependencies}

Recall the Simple OOTA example in Listing~\ref{lst:Simple OOTA} on
page~\pageref{lst:Simple OOTA},
in which T0 loads the value of \co{x} and stores it in \co{y} while T1
does the reverse.
A sufficiently perverse globally optimizing loose C++ compiler might
transform that program to the following before translating it into
machine code:
% Constructed by hand.
\begin{quote}
\small
\begin{tabular}{r|l|l}
% & \multicolumn{1}{c}{T0} & \multicolumn{1}{c}{T1} \\
% \hline
1. &
\texttt{~~int r1 = 42} &
	\texttt{~~int r2 = 42} \\
2. &
\texttt{~~y = r1} &
	\texttt{~~x = r2} \\
\end{tabular}
\end{quote}
The loads previously on line~1 have been replaced by constants.
Such a transformation complies with the loose C++ standard,
even though the resulting executable file would produce an unintuitive
OOTA outcome every time it runs!

The only justification a compiler could have for generating output like
this is that it knows exactly what accesses will be performed by both
threads, and therefore it knows that it will not violate the loose C++
memory model by assuming each thread's load reads from the other's
store.\footnote{
	A less perverse compiler could choose to avoid the
	OOTA cycle simply by not making this transformation.}
A similar justification can underlie the reasoning in
Section~\ref{sec:Semantic Dependencies Affected by Cross-Thread Optimizations};
in principle an analysis of the complete program could lead a compiler
to conclude that \co{y} will always be equal to \co{z} whenever a
particular \co{y - z} expression is evaluated, allowing the compiler
to replace the expression with a constant \co{0}.

By contrast, a compiler that analyzes only one thread at a time when
performing its optimizations and other code transformations will not
have this kind of global knowledge, and consequently it would not
perform the OOTA-ful transformation shown here.

Because we seek to find characteristics of compilers that will
guarantee the absence of OOTA behavior in the machine code they generate,
we will for now confine our attention to compilers that analyze only
one thread of source code at a time.
In more precise terms, we want the compilers under consideration
always to generate the same machine-code output for threads having
the same source code, regardless of the rest of the code in the programs
containing those threads.
Later on we will return to globally optimizing compilers.

\subsection{Inventing Atomic Loads Can Cause Errors and
Destroy Semantic Dependencies}
\label{sec:Inventing Atomic Loads Can Cause Errors and
Destroy Semantic Dependencies}

Invented atomic loads are problematic:
\begin{quote}
\begin{verbatim}
int r0 = x;
int r1 = r0 * r0 + 2 * r0 + 1;
\end{verbatim}
\end{quote}
For values of \co{x} small enough to avoid overflow, the abstract machine is
guaranteed to produce a perfect square in \co{r1}.

But if the compiler is permitted to invent atomic loads then the compiler
might transform this code as follows:
\begin{quote}
\begin{verbatim}
int r0 = x;
int invented = x;
int r1 = r0 * r0 + 2 * invented + 1;
\end{verbatim}
\end{quote}
If the load into \co{r0} happened when the value of \co{x}
was zero and the load into \co{invented} happened when the value of
\co{x} was 10, then the value of \co{r1} will be 21, which is not
a perfect square.
This fails to satisfy the abstract machine's guarantee
and therefore is an invalid transformation.

Inventing atomic loads can also result in OOTA-like behavior.
For example, consider this code, in which the final values of \co{r1}
and~\co{r2} are observable:
\begin{quote}
\begin{verbatim}
   int r1 = (x != 0);
   int r2 = (y != 0);
   z = (r1 == r2);
\end{verbatim}
\end{quote}
It is clear that the store to \co{z} semantically depends on the load
from \co{y}, because the value of \co{z} will change whenever \co{y}
changes between zero and nonzero (all else being equal).

However, an especially devious compiler might transform the source into
the following form before translating it to machine code:
\begin{quote}
\begin{verbatim}
 1. int r1;
 2. int r1a = (x != 0);
 3. int r1b = (x != 0);
 4. int r2 = (y != 0);
 5. if (r1a != r1b) {
 6.     r1 = r2;
 7.     z = 1;
 8. } else {
 9.     r1 = r1b;
10.     z = (r1 == r2);
11. }
\end{verbatim}
\end{quote}
The idea is that \co{r1a}, \co{r1b}, and \co{r2} can each be only zero or one,
so if \co{r1a} and \co{r1b} differ then one of them must be equal to \co{r2}.
In executions where this happens---because another thread writes to \co{x}
between the two loads---the implementation can choose at runtime to use
for \co{r1} whichever value agrees with \co{r2}, as shown on line~6.
Then the value stored to \co{z} on line~7 will simply be one,
with no dependence on the value loaded from \co{y}.

Therefore we should rule out transformations like these by insisting
the compiler not invent (or duplicate) atomic loads.
In fact, we will require that atomic accesses be treated as
``quasi volatile'', in that the compiler is allowed to omit,
merge, or reorder them but not invent them.

Just what does this mean?

\subsection{Volatile and Quasi Volatile Accesses}
\label{sec:Volatile and Quasi Volatile Accesses}

Declaring objects to be volatile is a way for the programmer to
indicate that the hardware should perform all accesses to these
objects exactly as written in the source code, perhaps because they
represent memory-mapped device registers or DMA buffers rather than
normal memory locations.
In any event, we expect compilers' translations of volatile-object accesses
into machine code to be as close to verbatim as possible.

To express this idea in more formal terms,
and to explain what we mean by ``quasi-volatile'' object accesses,
we augment the
requirements for a hardware execution $H$ to realize an abstract
execution $A$.
Each realization must include a map from the set of accesses of
volatile objects in $A$ to the set of instructions in $H$ that access
these objects, having the following properties:
\begin{itemize}
\item	The map connects accesses of the same type (loads to loads
	and stores to stores) and to the same object.
\item	The map connects accesses in a thread of $A$ to accesses in
	the corresponding thread of $H$.
\item	The map is value-preserving: The value of an access in $A$ must be
	the same as the value of the access it maps to in $H$.
\item	In benign environments the map must preserve the rf relation.
	That is, if volatile load $R$ in $A$ reads from store $W$ then
	then the instruction it maps to in $H$ must read from the
	instruction that $W$ maps to.
\item	The map is order-preserving: Two accesses in the same thread
	of $A$ must map to accesses occurring in the same order in $H$.
	(In other words, the compiler may not reorder accesses
	to volatile objects.)
\item	The map is onto: For every access in $H$ to a volatile object
	there must be an access in $A$ that maps to it.
	(In other words, the compiler may not invent accesses to
	volatile objects.)
\item	The map is one-to-one: Different accesses in $A$ map to
	different accesses in $H$.
	(In other words, the compiler may not merge accesses to
	volatile objects.)
\item	The map is total: Every access in $A$ to a volatile object
	maps to some access in $H$.
	(In other words, the compiler may not omit accesses to
	volatile objects.)
\end{itemize}
Most of these are direct consequences of the fact that volatile-object
accesses are considered to be a form of I/O when the program runs
in a nonbenign environment.
But to be clear, these requirements apply in all environments.

By contrast, accesses to quasi-volatile objects are normal memory
accesses, not subject to unpredictable interference in nonbenign
environments (otherwise the program's behavior would be undefined).
However, we do impose most of the requirements above on quasi-volatile
object accesses.
The last two bullet points are left out: Compilers are allowed to merge or
omit accesses to these objects.
Because of this, the bullet point about preserving the rf relation
applies only when $R$ is not omitted, in which case $W$ must not
be omitted either, but now it applies in all environments.
Lastly, the requirement for order preservation is weakened; it applies
only to pairs of accesses to the same quasi-volatile object.
Accesses to different objects may be reordered relative to each other.

These requirements prohibit
invention or duplication of atomic loads and stores.
However, they do permit omitting
redundant non-volatile atomic stores and fusing of non-volatile atomic
accesses to adjacent objects.

\section{Hardware Dependencies, Instruction Ordering, and the
Fundamental Property}
\label{sec:Hardware Dependencies, Instruction Ordering, and the
Fundamental Property}

\emph{Tagline: ``How to think about semantic dependencies.'' }

This section examines dependencies at the level of machine instructions
and then uses the mapping from the previous section to define the
Fundamental Property of semantic dependencies.

\subsection{Dependencies at the Hardware Level}
\label{sec:Dependencies at the Hardware Level}

Dependencies between machine instructions correspond to the flow of
information within a CPU.
Each instruction takes a set of inputs and provides a set of outputs,
some of which flow to inputs of later instructions.
The inputs determine what an instruction will do.

For example, an \co{add} instruction might have two inputs (the values to
be summed) and two outputs (the sum and some condition-code bits---e.g.,
Zero, Carry, and Overflow).
% \item	A conditional-move instruction would have as inputs the
%	condition-code bits to test, the source register value, and the
%	target register value; the output would be the value to be stored
%	in the target register.
% \item	A conditional- or computed-branch instruction would have as inputs
%	the condition-code bits to test, the address of the following
%	instruction (to be used if the condition is false) and the
%	destination address (to be used if the condition is true).
%	The output is the new address to be written to the
%	instruction-pointer register.
For another example, a memory-load instruction's input is the address
to load from, and its output is the value obtained by the load.
For a final example, a memory-store instruction's inputs are the value
to store and the address at which to store it; there are no outputs.

Using this scheme, we say that an instruction $J$ is dependent on
another instruction $I$ when any of $I$'s outputs flow into $J$'s inputs,
perhaps via intermediate instructions.
Tracing back, you can see that
any hardware dependency ultimately emanates from initial register values,
load instructions, immediate values, or I/O accesses.
We are setting I/O aside, which leaves load instructions as the only sources
of truly new information in a thread.

\subsection{Instruction Ordering}
\label{sec:Instruction Ordering}

A CPU may start executing an instruction speculatively, but at some
point it must either abandon that instruction or \emph{commit} it with
a set of specific outputs based on well-defined inputs.
A CPU is clearly not permitted to commit an instruction until its inputs
have been committed, unless the remaining uncommitted inputs cannot
affect that instruction's outputs.
For instance, a conditional-move instruction needn't wait for the source
input to commit once it knows that the condition is definitely false.

The overall effect of these hardware dependencies is that if a change to an
output of instruction $I$ would lead to a change in the action or outputs of
a later instruction $J$, then the CPU must commit $I$'s output before
committing $J$'s action and outputs.
Thus dependencies force the affected instructions to commit in order,
even on weakly ordered architectures.
We will say that one instruction is \emph{ordered after} another to mean that
it must commit after the other one commits.\footnote{
	Note that this implies nothing about when a load instruction
	retrieves its value from memory; it may do so long before it
	commits.}

Dependencies aren't the only ordering mechanism.
As mentioned in Section~\ref{sec:Brief OOTA Overview}, a CPU does
not make the value of a store instruction available to other CPUs
until after the store commits.
It then takes additional time for the stored value to travel to other
CPUs, owing to the finite speed of light and the non-zero size of
processor hardware.
Since a load instruction cannot commit until its output (the value
read) is fully determined, it must commit after the store it reads
from.

Conditional or computed branches also provide ordering.
An instruction executing after such a branch cannot commit
until the CPU has committed to whether the branch will be taken and
if taken, where it will branch to;
until then the CPU cannot know whether the later instruction should
be executed at all.
Therefore instructions following a conditional- or computed-branch
instruction must commit after the branch, and hence after the source
for the branch's condition and/or destination
input.\footnote{
	One exception is a branch that conditionally jumps to the
	next instruction.
	We ignore this possibility by treating such branches as no-ops.}

\subsection{The Fundamental Property of Semantic Dependencies}
\label{sec:The Fundamental Property of Semantic Dependencies}

We can now formulate the Fundamental Property that we want all semantic
dependencies to satisfy in implementations that treat all atomic objects
as volatile or quasi volatile.
\begin{quote}
Let $W$ be a store which semantically depends on loads
$\{R_0$, \ldots,~$R_n\}$ in some abstract execution $A$,
and suppose that $W$ is not
omitted in some hardware execution $H$ realizing $A$.
Then for some $i$, load $R_i$ is not omitted in $H$ and the
instruction it maps to is ordered before the instruction $W$ maps to.
\end{quote}

No implementation in which semantic dependencies satisfy the Fundamental
Property has a nontrivial OOTA cycle.
To see this, suppose that abstract execution $A$ having an OOTA cycle
is realized by hardware execution $H$.
This means there are atomic stores $W_i$ in $A$,
semantically depending on atomic loads
$\{R_{i,j}\}$ where each of the loads reads from one of the $W_k$
stores in a different thread.
Let $W_i'$ and $R_{i,j}'$ be the hardware instructions these accesses
map to in $H$, if they aren't omitted.
Assuming that the stores are not all omitted,
one of the $W_i'$ instructions, let's say $W_0'$, must commit first.
By the Fundamental Property, one of the loads that $W_0$ depends on,
let's say $R_{0,0}$, is not omitted and $R_{0,0}'$ is ordered before $W_0'$.
But now we have a contradiction:
(1)~$W_0'$ commits after $R_{0,0}'$;
(2)~$R_{0,0}'$ commits after the store instruction $W_k'$ it reads from;
and finally
(3)~$W_k'$ commits no earlier than $W_0'$.

If all the stores in the OOTA cycle are omitted then all the reads must
also be omitted, meaning that the entire cycle has been optimized away.
We conjecture that in this situation for full C++ there must be another
abstract execution which has the same observable effects as $A$ and is
also realized by $H$, but in which the OOTA cycle does not occur.
Thus there would be no way to tell, merely by observing the effects of
$H$, whether or not there was an OOTA cycle.
We therefore declare OOTA cycles in which all accesses are omitted to
be \emph{trivial}.

\section{A Definition of Semantic Dependency}
\label{sec:A Definition of Semantic Dependency}

\emph{ Tagline: ``How to think about OOTA.'' }

Semantic dependency is a notoriously difficult concept to define
rigorously and precisely.
A large part of the reason is because it was never a completely clear
concept to begin with, especially when there are multiple accesses to
the variables involved.
In this section we will stick our necks out by offering just such a
definition.
No doubt many people will object to it for various reasons, but we
nevertheless hope it will help move the discussion forward.

The definition given below is applicable only to C++ implementations
that treat all atomic objects as though they are volatile or
quasi volatile.
(For compilers that perform only per-thread analysis, not global
analysis, quasi volatile is sufficient.)
In this setting we can relate abstract and hardware executions by
means of the map of accesses described in
Section~\ref{sec:Volatile and Quasi Volatile Accesses}.
The key insight is that this allows us to consider semantic
dependencies at the level of the machine code, where they are much
more tractable.
This allows us to demonstrate that OOTA cycles cannot form
in the C++ implementations addressed by this paper.

\subsection{For Compilers Using Per-Thread Analysis}
\label{sec:For Compilers Using Per-Thread Analysis}

In this section we consider implementations whose compilers perform
only per-thread analysis and treat atomic objects as
quasi volatile.
This implies that if two different programs contain the same thread
(i.e., the same source code for the functions and objects in the thread), the
machine code generated by the compiler for the thread will be the
same in the two programs.

We begin by recognizing that semantic dependencies are relative to a
particular execution and a particular implementation,
as described in Section~\ref{sec:Properties of Semantic Dependencies}.
The same source code may or may not contain a semantic dependency,
according to the details of the execution in question and the
machine code produced by the compiler.
For this reason we will characterize semantic dependencies in a given
abstract execution realized by a given hardware execution.
(While it possible to argue about semantic dependencies in abstract
executions that have no hardware realizations, doing so seems pointless.)

Let $A$ be an abstract execution of some program $P$ containing a
thread $T$, and let $H$ be a hardware execution realizing $A$.
Let $W$ in $T$ be a store to an atomic object, and let $\{R_0,$
\ldots,~$R_n\}$ in $T$ be a set of loads from atomic objects on which
$W$ might or might not depend.
We can dispose of one case immediately: If $W$ is omitted in $H$ then
the issue of semantic dependency is moot.
You can give either answer since it will have no effect.
Therefore we'll assume that $W$ is not omitted.
Then:
\begin{quote}
There is a semantic dependency from $\{R_i\}$ to $W$ in $A$ and $H$,
relative to the compiler used to produce $H$, if there is another
abstract execution $B$ realized by hardware execution $G$ under the
same compiler that together \emph{witness the semantic dependency}.
\end{quote}

To be a proper witness, $B$ must be an execution of some program $Q$,
not necessarily the same as $P$ but which contains the same thread $T$.
The thread should start out with the same initial state in $A$ and
$B$, and all loads in $A$ coming before any of the $R_i$ should obtain
the same value as they do in $B$ (this is part of our interpretation
of ``all else being equal'').
It follows that the two abstract executions of $T$ will be identical
up to the first of the $R_i$ loads.

Let $W'$ and $R'_i$ be the accesses in $H$ that $W$ and the
non-omitted $R_i$ loads map to.
We then require that the hardware executions of $T$ in $H$ and $G$ be
identical for an initial period lasting up to the first of the $R'_i$.
Following this initial period there will be a common period, during
which $H$ and $G$ execute the same machine instructions but do not
necessarily compute the same values.
This common period ends when one of the hardware executions
takes a conditional branch that the other doesn't, or when a computed
branch leads to different addresses in the two executions, or when $T$
ends, whichever comes first.
Past this point $H$ and $G$ diverge and are no longer directly
comparable, as they execute different instructions.
Our third requirement for being a witness is that each load in the
common period either must obtain the same value in $H$ and $G$, or
must itself be one of the $R'_i$ loads, or must be ordered in $H$
after one of the $R'_i$ loads (this is the remaining part of our
interpretation of ``all else being equal''.)

Finally, we need to determine an instruction $X'$ in $G$ that
corresponds to $W'$.
If $W'$ is in the initial or common period of $H$ this is no
problem; we can take $X'$ to be $W'$ itself.
But if $W'$ is in the divergent part of $H$ then things aren't so
simple.
The choice is somewhat arbitrary, and so we will fall back on the
earlier proposal of matching up stores by the order they occur.
Let $y$ be the atomic object that $W'$ stores to, and suppose $W'$
is the $N$th store to $y$ within the divergent part of $H$.
Then $X'$ will be the $N$th store to $y$ in the divergent part
of $G$, if such a store exists.
Our last requirement for being a witness to a semantic dependency
is that $X'$ act differently from $W'$: it doesn't exist, it stores
a different value, or it stores to an object other than $y$.

\subsection{For Compilers Using Global Analysis}
\label{sec:For Compilers Using Global Analysis}

As promised earlier, we now consider implementations whose compilers
may use global analysis.
In order to obtain the desired results we have to require that these
compilers treat all atomic objects as volatile.
Equivalently, the machine code generated by such a compiler must be
the same for a given program as for a ``volatilized'' form of the
program in which all the atomic objects are defined to be volatile.

In this context our definition of semantic dependency is essentially
the same as before.
Since we can no longer expect the machine code for a thread to be the
same regardless of the program it belongs to, the program $Q$ in the
earlier definition (of which $B$ and $G$ are executions) must be
$P$ or its volatilized form.
However, we do now allow the possibility that the executions $B$ and
$G$ take place in a nonbenign environment.
Aside from these minor adjustments, the definition remains unchanged.

\subsection{Verifying the Fundamental Property}
\label{sec:Verifying the Fundamental Property}

Of course we want to check that our definition of semantic dependency
satisfies the Fundamental Property of
Section~\ref{sec:The Fundamental Property of Semantic Dependencies}.
Given the information we have already presented, the demonstration is
easy.

Suppose we have $W$, $\{R_i\}$, $A$, and $H$ as in the definition.
The Fundamental Property assumes that $W$ is not omitted in $H$, so
there is an abstract execution $B$ with hardware realization $G$
witnessing the semantic dependency of the store $W$ on the loads
$\{R_i\}$ in $A$ and $H$.
We must show that some $R_i$ is not omitted and $R'_i$ is ordered
before $W'$ in $H$.
The proof splits into three cases.

First case: $W'$ lies in the initial period of $T$ in $H$.
During the initial period of the hardware executions, $H$ and $G$
behave identically and therefore $W'$ performs the same write in
both.
This contradicts the fact that $B$ and $G$ witness the semantic
dependency.

Second case: $W'$ lies in the common period of $T$ in $H$.
Since the action of $W'$ in $H$ is different from its action in $G$,
at least one of its inputs must differ between the two hardware
executions.
Therefore the source instruction for that input must behave
differently, and so must one of its sources, going back until we reach
a load instruction that obtains differing values in $H$ and $G$.
Then $W'$ depends on this load and so is ordered after it.
And since the load must lie in the common period of $H$, by the
definition of semantic dependency it must either be one of the
$R'_i$ or be ordered after one of them.
Therefore $W'$ is ordered after one of the $R'_i$ in $H$,
which certainly means that $R_i$ is not omitted.

Third case: $W'$ lies in the divergent part of $T$ in $H$.
This happens when $W'$ comes after the conditional or computed branch
which marks the end of the common period by going different ways in
$H$ and $G$.
Just as in the previous case, since the branch behaves differently in
the two executions it must be ordered after one of the $R'_i$ loads.
And then so must $W'$, because any instruction following a conditional-
or computed-branch instruction must commit after the branch commits.
QED.

A corollary of this result is that if an implementation's compiler
either
(1)~uses per-thread analysis and treats atomic objects as quasi
volatile, or
(2)~uses global analysis and treats atomic objects as volatile,
then programs produced by that implementation will never exhibit OOTA.
Thus the implementation will automatically comply with full C++, even
though it may be been designed only to comply with loose C++.

\section{Issues and Refinements}
\label{sec:Issues and Refinements}

Here we consider some general questions related to our definition of
semantic dependency.
Additional information is available in a companion technical report,
which is not cited in order to maintain anonymity.
% P3064R2 ("How to Avoid OOTA Without Really Trying")
% https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3064r2.pdf

\subsection{Relative versus Absolute Dependency}
\label{sec:Relative versus Absolute Dependency}

One drawback of our definition of semantic dependency is that it is only
relative to a specific compiler or implementation.
This may strike some people as wishy-washy and avoiding the real
problem, in that a given piece of code should either contain or not
contain a semantic dependency, independent of the implementation used to
run it.

We can address this drawback by defining an \emph{absolute semantic
dependency} as one that is present relative to any valid loose C++
implementation, past, present, or future, real or imagined.
Of course this notion has its own problems, including that it is
extremely nonconstructive and impossible to apply in practice.
However it may be the best we can do with our current understanding
of computing systems.

There is one thing we can definitely state:
Programs produced by an implementation of the right sort will never
exhibit absolute OOTA (that is, an OOTA cycle in which all the
semantic dependencies are absolute).
This is simply because an absolute semantic dependency is {\it
a fortiori\/} a semantic dependency relative to the compiler in use.

But in fact we have shown more than this:
Programs will never exhibit an OOTA cycle relative to the compiler
used to build them, even when that cycle is not absolute.
In this sense we have gone beyond the requirement of full C++.

\subsection{Global Analysis and Volatile vs.\ Quasi Volatile}
\label{sec:Global Analysis and Volatile vs. Quasi Volatile}

In principle we don't need to require global-analysis compilers to
treat atomic objects as volatile; our results would still hold if they
merely treated them as quasi volatile.
We chose not to do this because it would violate our intuitions about
semantic dependencies.

For example, consider the Simple OOTA program shown in
Listing~\ref{lst:Simple OOTA}.
A loose C++ compiler using global analysis and treating \co{x} and \co{y} as
quasi-volatile objects could omit the two loads, replacing them in the
machine code with simple assignments ``\co{r1 = 42}'' and
``\co{r2 = 42}''.
This would be a valid transformation, and the resulting behavior would
not count as OOTA according to our definition because the
dependencies in T0 and T1 would not be semantic.

To see why not, recall that a semantic dependency must have a witness,
another execution in which the store acts differently.
But this transformed program has no other hardware executions; every time it
runs it will store 42 to both \co{x} and \co{y}.
(Keep in mind also that since the atomic objects are not treated as
volatile, they are not subject to unspecified interference when the
program runs in a nonbenign environment.)

This unintuitive behavior could not occur if the two loads were not
omitted.
In fact, the definition of semantic dependency might remain
perfectly acceptable if the requirement for global-analysis compilers
were weakened, if the compiler treated atomic objects as quasi
volatile and in addition was not allowed to omit accesses to them.
This is a possible topic for future research.

\subsection{Effect of Memory Layout}
\label{sec:Effect of Memory Layout}

Part of our demonstration of the Fundamental Property of semantic
dependencies relies on the fact, stated in
Section~\ref{sec:Abstract Executions},
that an abstract execution of a thread is entirely determined by the
values obtained for its loads.
But when we compare abstract executions of the same thread in two
different programs, this may no longer be entirely true owing to the
effect of differing memory layouts.

Consider this simple example:
\begin{quote}
\begin{verbatim}
x = (int) &x;
\end{verbatim}
\end{quote}
Even though the example contains no loads at all, it may store
different values when running in different programs because the
object \co{x} may be allocated at differing addresses in those
programs.
According to our definition, this could count as a degenerate OOTA
cycle of length one, in which the store is semantically dependent on
an empty set of loads!

To rule out such pathological counterexamples we should require that
in a witness to a semantic dependency, the addresses of all the
objects and functions referred to in the thread $T$ are the same as
in the original execution.
This is a very technical restriction but there are occasions when
the issue might realistically arise, such as when computing a hash
value based on an object's address.

\subsection{Merging Quasi-Volatile Loads}
\label{sec:Merging Quasi-Volatile Loads}

The compiler is permitted to merge quasi-volatile loads.
This can lead to surprising results because a particular load
may be merged with an earlier load in one execution and with a
later load in another.
This is demonstrated in the following, which is a variant of
the example in
Section~\ref{sec:Inventing Atomic Loads Can Cause Errors and
Destroy Semantic Dependencies}:
\begin{quote}
\begin{verbatim}
int r1 = (x != 0);
int r2 = (x != 0);
int r3 = (x != 0);
int r4 = (y != 0);
z = (r2 == r4);
\end{verbatim}
\end{quote}
Consider an abstract execution in which \co{r1}, \co{r2}, and \co{r4}
are zero and \co{r3} is one (because another thread changed the value
of \co{x} between two of the loads).
We would expect that the store to \co{z} would be semantically
dependent on the load of \co{y} in this execution.

However, a per-thread analysis compiler can translate this into the
machine-code equivalent of:
\begin{quote}
\begin{verbatim}
int r1 = (x != 0);
int r2;
int r3 = (x != 0);
int r4 = (y != 0);
if (r1 != r3) {
   r2 = r4;
   z = 1;
} else {
   r2 = r1;
   z = (r2 == r4);
}
\end{verbatim}
\end{quote}
In effect, the \co{r2} load is merged with the \co{r1} load in
executions where \co{r1} is equal to \co{r3} or to \co{r4}, and it is
merged with the \co{r3} load in other executions.

To demonstrate the semantic dependency in the original code,
a suitable witness would have
to include a hardware realization of the abstract execution in which
\co{r1} and \co{r2} are zero and \co{r3} and \co{r4} are one.
But there are no hardware realizations of this execution with the
machine code indicated above!
Since \co{r1} is different from both \co{r3} and \co{r4}, the \co{r2}
load will be merged with the \co{r3} load and so \co{r2} will
necessarily be one, not zero.
Thus, relative to this compiler the example does not contain a
semantic dependency.

Although it is unexpected, we cannot say that this conclusion is
definitely wrong, because our intuitive notions of semantic
dependency are not clear in cases where multiple loads of the same
variable are present.
% Can we instead say that transforming the code might require that the
% witness also be transformed?

\subsection{Other Potential Refinements}
\label{sec:Other Potential Refinements}

Additional future work could:
\begin{itemize}
\item	Extend these results to some classes of interpreters and JITs
	on one hand, and to link-time optimizations (LTO) on the other.
\item	Consider various classes of global analysis, such as
	demonstrating that a given expression will always evaluate
	to a constant.
\item	Examine the possibility of ``flattening'' optimizations that
	combine multiple threads into one.
\item	Explore cases in which semantic dependencies must be absolute,
	not relative to any particular compiler.
\item	Include the effects of input and output or other operations
	which can vary from one execution to another.
\item	Delineate more precisely the limits of permissible behavior
	for quasi-volatile object accesses.
\end{itemize}

\section{Summary and Conclusions}
\label{sec:Summary and Conclusions}

This paper focused solely on compilers, primarily those that do only
per-thread analysis and optimization.
It introduced the notion of quasi-volatile object accesses and gave
a definition for semantic dependency that is relative to a particular
C++ implementation.
It formulated a Fundamental Property of semantic dependency and proved
that implementations subject to some very mild restrictions on how
they treat atomic object accesses do have this property.
Finally, it demonstrated that satisfying the Fundamental Property
guarantees an implementation cannot give rise to OOTA executions.

The conclusion to be drawn from this work is that in this context,
avoiding OOTA cycles requires no changes to the standard,
to current implementions, or to user practices for portable code.

% \clearpage
% \appendix

% \section{But What About Tooling?}
% \label{sec:But What About Tooling?}
% @@@ State conclusions if needed where needed.

% \subsection{Load/Store Ordering: Hardware View for Software Hackers}
% \label{sec:Load/Store Ordering: Hardware View for Software Hackers}
% @@@ State conclusions if needed where needed.

% \subsection{Status Quo and Focused Tooling}
% \label{sec:Status Quo and Focused Tooling}
% @@@ State conclusions if needed where needed.

% \subsection{Change Relaxed to Forbid Load Buffering}
% \label{sec:Change Relaxed to Forbid Load Buffering}
% @@@ State conclusions if needed where needed.

% \subsection{Add Load-Store Memory Order that Forbids Load Buffering}
% \label{sec:Add Load-Store Memory Order that Forbids Load Buffering}
% @@@ State conclusions if needed where needed.

\bibliographystyle{plain}
% \addcontentsline{toc}{section}{References}
\bibliography{bib/RCU,bib/WFS,bib/hw,bib/os,bib/parallelsys,bib/patterns,bib/perfmeas,bib/refs,bib/syncrefs,bib/search,bib/swtools,bib/realtime,bib/TM,bib/standards,bib/memorymodel.bib}

\end{document}
