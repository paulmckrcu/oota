\documentclass[10]{article}

% standard packages

% A more pleasant font
\usepackage[T1]{fontenc} % use postscript type 1 fonts
\usepackage{textcomp} % use symbols in TS1 encoding
\usepackage{mathptmx,helvet,courier} % use nice, standard fonts for roman, sans and monospace respectively

% Improves the text layout
\usepackage{microtype}

\usepackage{lscape}
\usepackage{fancyhdr}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{url}
\usepackage{graphics}
\usepackage{enumerate}
\usepackage{ifthen}
\usepackage{float}
\usepackage{listings}
\lstset{basicstyle=\ttfamily}
% \usepackage[strings]{underscore}
% \usepackage{underscore}
\usepackage[bookmarks=true,bookmarksnumbered=true,pdfborder={0 0 0}]{hyperref}

\lstset{
  literate={\_}{}{0\discretionary{\_}{}{\_}}%
}

\usepackage[table]{xcolor}
\usepackage{booktabs}

\DeclareUrlCommand\email{}

\pagestyle{fancy}
\rhead{}

\newfloat{listing}{tbp}{lol}
\floatname{listing}{Listing}

\begin{document}
\title{D3064R0: OOTA Execution is Provably Vacuous}

\newcommand{\co}[1]{\lstinline[breaklines=yes,breakatwhitespace=yes]{#1}}

\author{
Paul E.~McKenney\\\email{paulmck@kernel.org} \and
Michael Wong\\\email{fraggamuffin@gmail.com} \and
Maged Michael\\\email{maged.michael@gmail.com} \and
The Indefatigible TBD
}
\date{November 27, 2023 (Pre-Tokyo)}
\maketitle{}

Audience: SG1

\begin{abstract}
	The out-of-thin-air (OOTA) properties of the specification
	of \co{memory_order_relaxed} have resulted in considerable
	consternation over the years.
	Attempts to create memory models that rule out OOTA behaviors
	have either been non-executable, complex, or unloved by compiler
	writers.
	But at the same time, there are no known instances of OOTA
	behavior in real C++ implementations.

	This paper goes further, adding temporal constraints from
	long-standing laws of physics and constraints on hardware
	architecture and design, thereby providing an informal
	proof that OOTA cannot occur in correctly constructed C++
	programs running on real-world systems.

	The result is that a correct C++ implementation necessarily
	preserves the semantic dependencies that prevent OOTA
	cycles from forming.
\end{abstract}

\section{Background}
\label{sec:Background}

This section provides a brief overview of the OOTA problem followed
by an equally brief summary of prior work in this area.

\subsection{Brief OOTA Overview}
\label{sec:Brief OOTA Overview}

OOTA occurs when a group of threads load from each others' stores,
but where each thread's store depends on the value returned by that
thread's load.
A given out-of-thin-air value passes around the resulting cycle.
Because there is a cycle, on a real system, at least one of the links
must be counter-temporal, that is, either a load returns a value before
that value is stored, or a store's address or value is computed before
a prior load returns a value used in that computation.

\begin{listing}[tbp]
\begin{verbatim}
 1 int main() {
 2   atomic_int x=0;
 3   atomic_int y=0;
 4   int r1;
 5   int r2;
 6   {{{ { r1=x.load(memory_order_relaxed);
 7         y.store(r1,memory_order_relaxed); }
 8   ||| { r2=y.load(memory_order_relaxed);
 9         x.store(r2,memory_order_relaxed); }  }}}
10   return 0;
11 }
\end{verbatim}
\caption{OOTA Cycle}
\label{lst:OOTA Cycle}
\end{listing}

Listing~\ref{lst:OOTA Cycle}
shows an OOTA cycle, where the \co{\{\{\{} on line~5 begins a set of
threads, the \co{|||} on line~8 separates a pair of threads, and
the \co{\}\}\}} on line~9 ends a set of threads.\footnote{
	This is pseudocode in the format of CPPMEM,
	which does not permit writes from non-local variables.}
Lines~6 and~7 do a relaxed copy from \co{x} to \co{y}, and
lines~8 and~9 to a relaxed copy from \co{y} to \co{x}.
Even though both \co{x} and \co{y} are initialized to zero on
lines~2 and~3, respectively, according to the mathematical core
of the C++ memory model, the only constraint on the final values
of \co{x} and \co{y} is that they be equal.
Counterintuitivily, their final values could be any member of their type.

\begin{listing}[tbp]
\begin{verbatim}
 1 int main() {
 2   atomic_int x = 0;
 3   atomic_int y = 0;
 4   volatile int r1;
 5   volatile int r2;
 6   {{{ { r1 = x.load(memory_order_relaxed);
 7         if (r1 == 1)
 8           y.store(1, memory_order_relaxed);
 9         else
10           y.store(r1, memory_order_relaxed); }
11   ||| { r2=y.load(memory_order_relaxed);
12         x.store(r2,memory_order_relaxed); }  }}}
13   }}};
14   return 0;
15 }
\end{verbatim}
\caption{OOTA Cycle Based on Conditionals}
\label{lst:OOTA Cycle Based on Conditionals}
\end{listing}

A similar effect can be achieved using conditional statements, as
shown in Listing~\ref{lst:OOTA Cycle Based on Conditionals}.

Note that C++ implementations are permitted to evaluate to a more
general notion of OOTA values in cases involving unspecified or undefined
behavior, such as use of uninitialized objects.
However, these situations do not involve OOTA cycles, but rather user
errors and other issues that can lead to unfriendly optimizations that
in turn result in unpredictable output.
This paper focuses instead on OOTA cycles.

C++ implementations may of course avail themselves of the as-if rule.
However, correct use of the as-if rule requires that the observable
behaviors of the program be appropriately preserved.

\subsection{Prior Work}
\label{sec:Prior Work}

All OOTA workers owe a debt to the foundational work in the infamous
``Causality Test Cases''.\footnote{
	\url{http://www.cs.umd.edu/~pugh/java/memoryModel/unifiedProposal/testcases.html}.}

Some executable C++ memory models correctly flagged executions involving
OOTA~\cite{JadeAlglave2014HerdingCats}.\footnote{
	Others cleverly avoid this issue by forbidding atomic
	stores of non-constant values~\cite{MarkBatty2011cppmem}.}
% @@@ Better herd7 C11 citation?
However, because these models are atemporal, they cannot reject
OOTA executions other than by flagging the OOTA value as arbitrary
(which some in fact do in at least some cases).

P0442R0 (``Out-of-Thin-Air Execution is Vacuous'')~\cite{PaulEMcKenney2016OOTA}
provided a decision procedure for classifying behaviors as permitted
misordering on the one hand or disallowed OOTA on the other, using
a perturbation method based on the insight that all OOTA behaviors are
fixed-point computations.

Some workers recommend avoiding OOTA by ordering prior relaxed
loads before subsequent relaxed
stores~\cite{Boehm:2014:OGA:2618128.2618134,HansBoehm2019OOTArevisitedAgain,Lahav:2017:RSC:3062341.3062352},
but this requires real instructions be executed, consuming real
time and real electrical power to solve a strictly theoretical
problem.

Other workers recommend various procedures to identify and avoid OOTA
cycles~\cite{Lahav:2017:RSC:3062341.3062352,Sinclair:2017:CAR:3079856.3080206,Lee:10.1145/3385412.3386010,MarkBatty2019ModularRelaxedDependenciesOOTA},
but none of these have been looked upon favorably by compiler implementers.

All this work focused on either identifying OOTA or on how C++
implementations could avoid it.
None of this work applied real-world temporal constraints to the OOTA
problem.
Which might explain why no known real-world C++ implementation results
in OOTA executions.

This paper therefore drops the question of how OOTA can be avoided and
instead focuses on proving that OOTA cannot occur.

\subsection{OOTA Definition}
\label{sec:OOTA Definition}

Prior work defines ``OOTA cycle'' by example, without a precise
definition.
Sometimes ``causal cycle'' is used as if it was a definition, but
without a clear decision process for what does and does not
constitute a causal cycle.
P0442R0 defines an OOTA cycle as a fixed-point computation that is
destroyed by perturbations, which makes perfect sense to that paper's
authors, but has left others unsatisfied.

We could follow this long-established tradition of proposing
solutions to the OOTA problem without a definition of this problem.
However, we (perhaps foolishly) felt the need to provide a clear
definition, which is a more detailed version of the wording in 33.5.4p8
(\co{[atomics.order]}):

\begin{quote}
	Implementations should ensure that no “out-of-thin-air” values
	are computed that circularly depend on their own computation.
\end{quote}

We therefore define an OOTA cycle (also known as a causal cycle) as a
cycle in the memory-model sense involving the load-buffering (LB) pattern.
Within each thread in the cycle, the value stored depends on
the value loaded.
This dependency might be a data dependency
(see Listing~\ref{lst:OOTA Cycle})
or it might be a control dependency
(see Listing~\ref{lst:OOTA Cycle Based on Conditionals}).\footnote{
	Sufficiently motivated readers should have no problem constructing
	examples involving address dependencies.}
There must be at least two distinct values such that if a given load
returns a given value, that value forces the cycle to execute so as to
cause the corresponding store's value to be equal to that given value, and
for that store to occur early enough for the load to return that value.

\section{Real-World Constraints}
\label{sec:Real-World Constraints}

Real-world constraints are imposed by the standard, which have been
considered in prior work, and by the laws of physics, which have
not.
Additional constraints are imposed by hardware architecture and
design, which have been partially accounted for in the standard,
one limitation being that accurate and executable formal descriptions
of hardware memory models did not appear until after the standard
was released.

The following sections discuss these constraints, starting with the
laws of physics, continuing with hardware constraints, and ending
with constraints imposed by the standard.

\subsection{Laws of Physics}
\label{sec:Laws of Physics}

Evidence to date suggests that the universe is
causal~\cite{Plato360BC-causality},
the speed of light is finite~\cite{OleRoemer1671SpeedOfLight}, and
atoms are of non-zero size~\cite{JeanBaptistePerrin1923AtomSize}.
These fundamental laws of physics are empirical in nature, but
are backed by a great many observations extending back to a time
preceding electronic computers, let alone computer languages that
support concurrency.

The relationship between these three laws of physics and OOTA
cycles is described in the following sections.

\subsubsection{Causality}
\label{sec:Causality}

Plato's ``Timaeus'' notwithstanding, there has been much recent debate
as to whether the universe is in fact causal with cause necessarily
preceding effect.
However, no one has managed to construct a real time-travel machine
or any other apparatus that might result in effect preceding cause,
as discussed at length in the Wikipedia article on retrocausality and
its many references.\footnote{
	\url{https://en.wikipedia.org/wiki/Retrocausality}.}

This paper will therefore assume that effect cannot precede cause in
any real-world computing system.

\subsubsection{Finite Speed of Light}
\label{sec:Finite Speed of Light}

If the speed of light was infinite, then information could be transmitted
at infinite speed, arriving at any destination, no matter how remote,
in zero time.
This situation would permit a load to return the value stored by some
other thread at the exact instant that the store executed, which
would in turn remove propagation delay as a factor preventing
causal loops such as OOTA cycles from forming.

However, all macroscopic measurements to date have shown the speed of
light to be finite, thus preventing causal loops based on infinite-speed
propagation of information.

This paper will therefore assume that the speed of light is finite, and
thus the maximum speed at which information can propagate is also finite.

\subsubsection{Non-Zero Sized Atoms}
\label{sec:Non-Zero Sized Atoms}

If atoms were of zero size, a zero-sized computer might be constructed.
In such a computer, even give finite speed of light, information could
propagate among computing elements in zero time.
This situation would also permit a load to return the value stored by
some other thread at the exact instant that the store executed, which
would again in turn remove propagation delay as a factor preventing
causal loops such as OOTA cycles from forming.

However, there has to date been no successful attempt to construct a
zero-sized computing device capable of running non-trivial C++ programs.

This paper will therefore assume that any computing device capable of
running C++ will be of non-zero size.

\subsubsection{Laws of Physics: Consequences}
\label{sec:Laws of Physics: Consequences}

These three laws of physics mean that if one thread loads the value
stored by some other thread, that load must have occurred later
in global time than did the store.
In other words, the reads-from relation is temporal in nature.

In contrast, the C++11 modification-order relation is atemporal,
because computing systems can and observably do determine the
modification order of a set of concurrent stores long after the fact.
This means that the store that executed earlier in global time
might appear later in the stored-to object's modification
order~\cite{McKenney20xxParallelProgramming}.
The atemporal nature of C++11 modification order is due to the hardware
store-buffer optimizations used by modern multicore systems.

This atemporality is also why cycles are not considered to have
OOTA cycles when at least one link from one thread to the next
involves both threads storing to the same object.
Similarly, cycles having at least link where one thread loads from an
object and the next thread stores to that object are also not considered
to be OOTA cycles.

Current programming-language abstract machines do not capture these
temporal constraints, which makes it more difficult for them to
efficiently rule out OOTA behavior.

\subsection{Hardware Architecture and Design}
\label{sec:Hardware Architecture and Design}

In happy contrast to the situation at the start of the C++11 memory-model
effort, heavily used CPU families now have accurate and executable formal
memory models, all of which prohibit the causal cycles required to form
OOTA cycles.
Even more important, there is clarity on exactly what hardware can and
can not do in the course of speculative execution.
Finally, computation is based on instructions, and instructions require
finite time to execute, even when executed speculatively.

\begin{listing}[tbp]
\begin{verbatim}
 1 int foo(int i) {
 2   atomic_int x = 0;
 3   int y[3];
 4   volatile int r1;
 5   {{{ { y[i].store(42, memory_order_relaxed);
 6         x.store(1, memory_order_relaxed); }
 7   ||| r1 = x.load(memory_order_relaxed);
 8   }}};
 9   return 0;
10 }
\end{verbatim}
\caption{Speculated Store and Non-Speculated Load}
\label{lst:Speculated Store and Non-Speculated Load}
\end{listing}

For example, in Listing~\ref{lst:Speculated Store and Non-Speculated Load},
suppose that the storage for \co{x} is located immediately before that
for \co{a[]}.
Suppose further that hardware speculation incorrectly guesses the value
of \co{i} to be -1, so that the speculated store of 42 on line~5 uses
the address of \co{x}.\footnote{
	Yes, \co{y[-1]} is undefined behavior, but the CPU neither knows
	nor cares, nor should it.}
Then line~7 can cause the final value of \co{r1} to be 42, when
by the rules of the C++ abstract machine the value must instead be
either zero or one.

Therefore, non-speculative loads are prohibited from loading values from
speculative stores.

As a special case of this prohibitino, a value stored speculatively must
not be exposed to non-speculative loads from other threads.
In the further special case of multiple hardware threads sharing a core,
one might imagine speculation involving multiple threads.
However, in this case, any squashing of speculation must squash the
full extent of that speculation, across all of the hardware threads
that were involved.

Finally, the prohibition of hardware OOTA applies within a single
multi-threaded core as well as between cores.
This means that for any set of speculative accesses to be committed,
all loads in that set must have been confirmed by corresponding
non-speculative stores.

\subsection{Constraints of the Standard}
\label{sec:Constraints of the Standard}

Because C++ volatile atomics constitute observable
behavior~\cite[\co{intro.abstract}]{ThomasKoeppe2022N4910},
they must be executed in strict accordance with the rules of the abstract
machine.
This means that any C++ volatile atomic operation involving a store
must execute as if all loads whose return values are used to compute
that store's address or value have already returned.
OOTA afficienados will recognize this as a special case of ordering
relaxed loads before relaxed stores, albeit one not requiring
expensive memory-fence instructions on weakly ordered architectures.

Although non-volatile accesses to atomic objects are not observable behavior,
they are still subject to the memory model.
In particular, implementations must not behave as if they have:

\begin{enumerate}
\item	Invented atomic stores.
\item	Duplicated atomic stores.
\end{enumerate}

\paragraph{Invented Atomic Stores}

\begin{listing}[tbp]
\begin{verbatim}
 1 int main() {
 2   atomic_int x = 2;
 3   volatile int r1;
 4   {{{ { x.store(42, memory_order_relaxed); // invented store
 5         x.store(3, memory_order_relaxed); }
 6   ||| r1 = x.load(memory_order_relaxed);
 7   }}};
 8   return 0;
 9 }
\end{verbatim}
\caption{CPPMEM Invented Store}
\label{lst:CPPMEM Invented Store}
\end{listing}

The reason that atomic stores cannot be invented is that doing so can
introduce new (and almost certainly undesirable) behaviors that are
forbidden by the abstract machine.
To see this, consider Listing~\ref{lst:CPPMEM Invented Store},
a CPPMEM\footnote{
	\url{http://svr-pes20-cppmem.cl.cam.ac.uk/cppmem/index.html}.}
litmus test that demonstrates such a behavior.
Without line~4, only the values~2 and~3 can be stored to \co{r1}.
With that line, the additional value~42 can also be stored to \co{r1}.
The compiler is therefore forbidded from inventing that store of 42
unless it can prove that doing so does not negatively affect the
program's observable behaviors, independently of OOTA cycles.
For example, the implementation might be able to prove that there
are no other accesses to \co{x} at the time of the invented store.

\paragraph{Duplicated Atomic Stores}

\begin{listing}[tbp]
\begin{verbatim}
 1 int main() {
 2   atomic_int x = 0;
 3   volatile int r1;
 4   {{{ { x.store(1, memory_order_relaxed); // duplicated store
 5         x.store(1, memory_order_relaxed); }
 6   ||| { r1 = x.load(memory_order_relaxed);
 7         x.store(2, memory_order_relaxed); }
 8   }}};
 9   return 0;
10 }
\end{verbatim}
\caption{CPPMEM Duplicated Store}
\label{lst:CPPMEM Duplicated Store}
\end{listing}

Duplicating atomic stores can also introduce new and undesireable
behaviors.
To see this, consider Listing~\ref{lst:CPPMEM Duplicated Store}, a CPPMEM
litmus test that demonstrates such a behavior.
Without line~4, if the final value of \co{r1} is 1, then the final value
of \co{x} must be 2.
With that line, the final value of \co{x} can be 1 even when the
final value of \co{r1} is 1.
The compiler is therefore forbidded from duplicating that store of 1
unless it can prove that doing so does not negatively affect the
program's observable behaviors, independently of OOTA cycles.
Again, the implementation might be able to prove that there
are no other accesses to \co{x} at the time of the invented store.

\paragraph{Omitted Atomic Stores}
In contrast, an implementation is permitted to omit atomic stores
under less restricted circumstances.
For example, a pair of back-to-back stores to \co{x} might always be
executed such that no other thread accesses \co{x} during the time
between those two stores.
This means that, short of inspecting the assembly code, the user has no
way of proving that the store was in fact omitted.
The user can always resort to volatile atomic stores or inline
assembly\footnote{
	For example, by placing the Linux-kernel \co{barrier()} macro
	between the two stores.
	This macro is an empty GCC \co{asm} that specifies the \co{memory}
	clobber.}
to prevent the compiler from omitting a store.
However, omitting a store cannot create an OOTA cycle.

\section{How to Form an OOTA Cycle?}
\label{sec:How to Form an OOTA Cycle?}

Rather than look at how to prevent an implementation from forming
an OOTA cycle, this section instead looks at how to form one.
This will lead to the conclusion that a correct C++ implementation
cannot form an OOTA cycle.

Again, an OOTA cycle consists of loads from other threads' stores where
a given store's address or value depends on the value returned by
the corresponding thread's prior load.
All of the links from ont thread to the next are reads-from links, which
are temporal in nature, that is, a given store must execute earlier in
global time than any load from that stored-to object that returns the
value stored.

And again, an OOTA cycle must have at least one link that goes backwards
in time.
The only possible candidate counter-temporal links so are those
confined to a given thread.
This in turn requires that a store that depends on an earlier load
be executed earlier in time than that load.
The two ways that this can happen are when the implementation:

\begin{enumerate}
\item	Is able to determine (prove) the location and value of the store.
\item	Guesses the location and value of the store and has some means
	to take corrective action should any guess prove incorrect.
\end{enumerate}

Each of these possibilities is discussed in the next sections.
followed by a section on the possibility of flattening a multithreaded
program into a single thread.

\subsection{Implementation Proves Value}
\label{sec:Implementation Proves Value}

Implementation can sometimes prove the location and value of a given
store.
For example, if the value loaded is multiplied by zero, the result
is known to be zero, so that the store can proceed prior to the load.
But in this case, there is no perturbation of the value returned from
the load that can affect the value stored.
Therefore, by P0442R0, that store cannot possibly take part in an
OOTA cycle.

\subsection{Implementation Guesses at Value}
\label{sec:Implementation Guesses at Value}

Hardware speculative execution is commonplace, and permits the
hardware to guess at values, squashing the speculation if any
of the guesses prove incorrect.
But in this case, a given store is not committed until all guesses that
this store depends on have been confirmed, and thus no uncommitted store
is visible to any other thread.
Therefore, hardware speculation cannot result in OOTA cycles, and
by design, even on weakly ordered
systems~\cite{ARMv7A:2010,ARMv8A:2017,PowerISA2.07-2013}.
This same constraint applies to software, which might use compare-and-swap
loops or hardware transactional memory to confirm guesses.

Either way, the need to wait for guesses to be confirmed before committing
stores prevents those stores from becoming visible prior to the
completion of any loads that those stores depend on.

\subsection{But What About Flattening?}
\label{sec:But What About Flattening?}

The concept of flattening multiple threads into a single thread is
tantalizing, especially given the large cache-miss penalties inherent
to large multicore systems, which can have many sockets containing
thousands of CPUs.
One might object to the whole concept of flattening as a compiler
optimization given the difficulty inherent in avoiding deadlocks
and other hazards.
Nevertheless, it is only reasonable to ask whether flattening can
result in OOTA cycles.
As we will see, the answer is ``no''.

But for the purposes of OOTA cycles, the flattening situation is
much simpler.
Any fully flattened program will be single threaded, which implies that
each of its atomic load operations will return the last value stored to
that same object in sequenced-before order (or the initial value if there
is no such store).
Thus, any sequence of loads and stores must have a first operation and
a last operation, and the last operation cannot affect the first
operation.
Therefore, cycles cannot possibly form in a flattened program,
which in turn implies that OOTA cycles cannot form.

\subsection{Illustrative Examples}
\label{sec:Illustrative Examples}

This section works through OOTA examples, showing the effects of
the constraints called out in this paper.

\subsubsection{OOTA Cycle}
\label{sec:OOTA Cycle}

Listing~\ref{lst:OOTA Cycle}
shows the canonical example of an OOTA cycle.
Because values from speculative stores cannot be visible to other threads,
line~7 cannot be committed until after line~6 completes.
Similarly, line~9 cannot be committed until after line~8 completes.

Because values stored take non-zero time to propagate from one thread
to another, no cycle can form, and thus no OOTA cycle can form.

\subsubsection{OOTA Cycle Based on Conditionals}
\label{sec:OOTA Cycle Based on Conditionals}

Listing~\ref{lst:OOTA Cycle Based on Conditionals}
shows an if-condition-based example of an OOTA cycle.
Because values from speculative stores cannot be visible to other threads,
lines~8 and~10 cannot be committed until after the load in line~6 completes
and the conditional on line~7 is evaluated non-speculatively.
Similarly, line~12 cannot be committed until after line~11 completes.

Because values stored take non-zero time to propagate from one thread
to another, no cycle can form, and thus no OOTA cycle can form.

Note that if both legs of the \co{if} statement spanning lines~7-10,
the compiler would be within its rights to hoist the store out of
that \co{if} statement.
However, in this case, the two stores on lines~8 and~10 are storing
distinct values, preventing the compiler from hoisting.

Note also that some architectures provide conditional-move instructions,
which allows line~7 to avoid emitting a conditional branch.
However, such architectures would still treat a store of the
target of the conditional-move instructions as speculative until
such time as the conditional moves (and any loads that they depend
on) complete.

\subsection{Correct Implementations Cannot Form OOTA Cycles}
\label{sec:Correct Implementations Cannot Form OOTA Cycles}

This section has shown that an implementation that correctly
handles the memory model, observable behavior, and the as-if rule
cannot form OOTA cycles.

\section{Suggested Changes to Wording}
\label{sec:Suggested Changes to Wording}

Add the following note in \co{[atomics.order]} after 33.5.4p9:

\begin{quote}
	[ \emph{Note:} Implementations that respect the following
	constraints are incapable of computing out-of-thin-air value
	that depend circularly on their own computation:

	\begin{itemize}
	\item	Correctly implement the C++ abstract machine, and
	\item	Run on correctly implemented hardware
		(specifically, hardware that forbids non-speculative
		loads from returning values from speculative stores
		and that forbids exposing values from speculative
		stores to other threads), and
	\item	Reside in a universe having a finite speed of light
		and atomic matter.
	\end{itemize}

	All three constraints are subject to the as-if rule.
	For example, those creating an alternate universe in which
	C++ implementations are to run correctly should ensure
	that their universe behaves as if the speed of light was
	finite and the nature of matter was atomic.
	--- \emph{end note} ]
\end{quote}

\section{Conclusion}
\label{sec:Conclusion}

This paper has added temporal reasoning to OOTA analysis of the C++
memory model.
This work shows that correct C++ implementations are inherently incapable
of producing OOTA cycles.

Therefore, if your C++ implementation produces OOTA cycles, you have
bugs that would be considered bugs even if OOTA cycles were allowed.
And you need to fix those bugs, regardless of whether they are in your
compiler, your hardware, or your alternative universe.

\section{Acknowledgments}
\label{sec:Acknowledgments}

We are grateful to David Goldblatt and Jade Alglave for their careful
review of an early draft of this paper and to John Wikerson for asking
Paul for a rant and taking the proffered rant seriously.
We also owe David Goldblatt a debt of gratitude for his having asked an
insightful question at the right time, and for his ``Deathstation 9000''
demonic CPU.
Nonetheless, all errors and omissions in this paper are the sole property
of the authors.

% To do:
% Luc Maranget: Memory models.
% Richard Gristenthwaite: ARM hardware.
% Mark Rutland: ARM memory ordering.
% Will Deacon: ARM hardware, C++, and OOTA.
% Greg Kroah-Hartman and Linus Torvalds: FYI, trouble being caused.
% Christoph Hellwig: Why slow on BPF memory model.
% Miguel Ojeda: LKMM and Rust.
% Alice Ryhl: LKMM and Rust.
% Ralf Jung: Rust.
% Catalin Marinas: ARM and formal methods (qspinlock).
% Segher Boessenkool:  GCC.
% Dan Lustig: NVIDIA memory ordering.
% Andrea Parri: Memory models, RISC-V hardware.
% Jonas Oberhauser: Memory models.
% Hernan Ponce de Leon: Memory models.
% Palmer Dabbelt: RISC-V hardware.
% Ali Sezgin: P0422R0.
% Tony Tye: P0422R0.
% Other memory-model maintainers, including Peter Zijlstra.
% Derek Williams: PowerPC hardware.
% Hans Boehm: Skeptic, memory models, OOTA.  Suggestions for others?
% Olivier Giroux: Skeptic, memory models, NVIDIA.
% Luke Geeson: Whatever...  Review.
% Ori Lahav: Prior OOTA.
% Viktor Vafeiadis:  Prior OOTA.
% Derek Dreyer: Prior OOTA.
% Brian Demsky: Prior OOTA.

% @@@ Inter-CPU speculation if inter-CPU squashing of speculation.

% Done as of November 30, 2023:
% David Goldblatt
% Boqun Feng, Uladzislau Rezki, Neeraj Upadhyay, Joel Fernandes, and
%	Frederic Weisbecker (RCU proteges)
% Michael Wong
% Maged Michael
% John Wickerson: Thank you.  OK for Peter Sewell, Mark Batty, etc.
% Akira Yokosawa: Professional courtesy, perfbook memory models.
% Dan Kelley, Alexei Starovoitov, Mykola Lysenko: FYI.  OOTA dropped
% Alan Jeffrey: Fixed-point insight P0422R0.  Reached out via LinkedIn.
% Jade Alglave: C++ herd model?  Memory models and variety of hardware.
% Alan Stern: Memory models, mathematical logic, and variety of hardware.


% \appendix

% \section{Demonstration of OOTA-Cycle Definition}
% \label{app:Demonstration of OOTA-Cycle Definition}

% \section{History}
% \label{sec:History}

\bibliographystyle{plain}
\bibliography{bib/RCU,bib/WFS,bib/hw,bib/os,bib/parallelsys,bib/patterns,bib/perfmeas,bib/refs,bib/syncrefs,bib/search,bib/swtools,bib/realtime,bib/TM,bib/standards,bib/memorymodel.bib}

\end{document}
